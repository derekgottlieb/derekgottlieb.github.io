    <!DOCTYPE html>
<html lang="en-us">
	<head>
		<meta charset="utf-8">
		<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<meta name="author" content="D.G.">
		<meta name="description" content="">
		<meta name="generator" content="Hugo 0.82.1" />
		<title>NUMA and torque/Moab &middot; Thoughts</title>
		<link rel="shortcut icon" href="https://thoughts.derekgottlieb.com/images/favicon.ico">
		<link rel="stylesheet" href="https://thoughts.derekgottlieb.com/css/style.css">
		<link rel="stylesheet" href="https://thoughts.derekgottlieb.com/css/highlight.css">
		<link rel="stylesheet" href="https://thoughts.derekgottlieb.com/css/dbgcustom.css">
                
    <link rel="icon" type="image/png" href="https://derekgottlieb.com/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="https://derekgottlieb.com/android-chrome-192x192.png" sizes="192x192">
    <link rel="icon" type="image/png" href="https://derekgottlieb.com/favicon-96x96.png" sizes="96x96">
    <link rel="icon" type="image/png" href="https://derekgottlieb.com/favicon-16x16.png" sizes="16x16">
	</head>

    <body>
       <nav class="main-nav">
	
		<a href='https://thoughts.derekgottlieb.com/'> <span class="arrow">←</span>Home</a>
	

	

	
		<a class="cta" href="https://thoughts.derekgottlieb.com/atom.xml">Subscribe</a>
	
</nav>


        <section id="wrapper">
            <article class="post">
                <header>
                    <h1>NUMA and torque/Moab</h1>
                    <h2 class="headline">
                    May 9, 2013 
                    <br>
                    
                    
                        
                            <a href="https://thoughts.derekgottlieb.com/tags/hpc">hpc</a>
                        
                            <a href="https://thoughts.derekgottlieb.com/tags/numa">numa</a>
                        
                            <a href="https://thoughts.derekgottlieb.com/tags/torque">torque</a>
                        
                            <a href="https://thoughts.derekgottlieb.com/tags/moab">moab</a>
                        
                            <a href="https://thoughts.derekgottlieb.com/tags/sysadmin">sysadmin</a>
                        
                            <a href="https://thoughts.derekgottlieb.com/tags/rants">rants</a>
                        
                            <a href="https://thoughts.derekgottlieb.com/tags/work">work</a>
                        
                    
                    
                    </h2>
                </header>
                <section id="post-body">
                    <p>TL;DR Support for NUMA systems in torque/Moab breaks existing means of
specifying shared memory jobs and limits scheduling flexibility in
heterogeneous compute environments.</p>
<h2 id="torque-numa-implementation">Torque NUMA implementation</h2>
<p>For torque NUMA, you manually define your NUMA nodes within a system, which
gives you some flexibility for how topology information is exposed to the queue
system.  The provided mom_gencfg utility defaults to defining a node per CPU
sockets, but you can manually craft your mom.layout file to aggregate sockets
together into larger NUMA nodes if desired.</p>
<p>While you run a single pbs_mom on the SMP, you&rsquo;ll see that torque tracks one
&ldquo;compute node&rdquo; per NUMA node in your system. To illustrate, if you have a
4-socket system with the hostname &ldquo;smallsmp&rdquo;, you may see the following:</p>
<pre><code>&gt; pbsnodes -l
smallsmp-0 offline
smallsmp-1 offline
smallsmp-2 offline
smallsmp-3 offline
</code></pre>
<p>There are some interesting side effects to this approach. For one, yes you can
offline individual sockets. The other side of that is you have to offline all
of the sockets if you&rsquo;re draining the system for maintenance. I think this also
highlights why you may not want to do this on anything except your big SMPs
(e.g., you have 1000 dual socket boxes, you suddenly see 2000 every time you
type pbsnodes).</p>
<h2 id="submitting-shared-memory-jobs">Submitting shared memory jobs</h2>
<p>In our environment, we use two ways to submit shared memory jobs given our Moab
configuration. We typically do &ldquo;qsub -l nodes=1:ppn=N&rdquo; to request N cores on a
single shared memory node. I&rsquo;ve used tpn as well since I found the
interpretation of ppn a little unintuitive if you use something other than
&ldquo;nodes=1&rdquo;. For distributed memory jobs, we use &ldquo;nodes=N&rdquo; and Moab is free to
distribute or pack cores across systems (we aim to maximize throughput over
single job performance and let jobs share nodes).</p>
<h2 id="moab-numa-limitations">Moab NUMA limitations</h2>
<p>In theory, Moab supports torque NUMA integration. But it is currently unable to
grok that it&rsquo;s dealing with an SMP and enforces unnecessary limitations if you
specify a shared memory job using ppn/tpn. For example, if I have 8 cores per
socket in my SMP and I configure a NUMA node per socket, then I can only run
jobs requesting up to &ldquo;ppn=8&rdquo;.  If I request any more than that, Moab won&rsquo;t
schedule them because it is unable to aggregate NUMA nodes within the same
system to run a shared memory job submitted in this manner.  Torque/Moab really
treat your NUMA SMP as a distributed memory cluster.</p>
<p>The way they address this is by telling you to submit shared memory jobs like
you would distributed memory jobs, but tack on some extra flags to constrain it
to one of your SMPs (e.g., &ldquo;-l nodes=N,flags=sharedmem&rdquo;).  This is less than
ideal for our HPC environment, and Adaptive has been unable to recommend a
satisfactory workaround for us since we raised the issue with them 4 months
ago.</p>
<h2 id="our-environment-and-numa-scheduling-limitations">Our environment and NUMA scheduling limitations</h2>
<p>Our HPC environment consists of two clusters managed by a single torque/Moab
instance. One is a commodity cluster that&rsquo;s a mix of 8-core and 16-core nodes
with between 24 and 128GB of memory per node. The other is a small cluster of
SMPs with the largest having 256 cores (8 cores per socket) and 4TB of memory
in a single system image.</p>
<p>When possible, we install new software packages on both clusters. We then write
queue submit scripts for our users that abstracts away the details of the queue
system and let&rsquo;s them focus on the app and resources that they need for a
particular job (e.g., cores, memory, runtime). By setting it up this way and
managing it all with a single queue system instance, we are able to abstract
away where the job runs as long as it&rsquo;s installed on both clusters and allows
jobs to run on the first cluster that has the required resources available,
easily load balancing across the two clusters to keep our cores busy.</p>
<p>We unfortunately didn&rsquo;t realize the scheduling limitations imposed by
torque/Moab NUMA until after deploying it on our new SMP.  Previously, we&rsquo;d
written custom utilities that handled processor core allocation to jobs in a
NUMA topology-aware way and configured a single large node entry in the queue
system config.  We&rsquo;d hoped to move away from this toward using more
standard/supported tools, but didn&rsquo;t have the time/resources to adequately test
the new NUMA support to discover its shortcomings.</p>
<h2 id="how-we-submit-shared-memory-jobs-now">How we submit shared memory jobs now</h2>
<p>Here&rsquo;s what we have implemented currently in our submit scripts based on how
many cores a user requests for a shared memory app installed on both clusters:</p>
<ul>
<li>For up to 8 cores, &ldquo;-l nodes=1:ppn=N,partition=commodity:smp&rdquo;</li>
<li>For 9-16 cores, &ldquo;-l nodes=1:ppn=N,partition=commodity&rdquo;</li>
<li>For more than 16 cores, &ldquo;-l nodes=N,partition=smp&rdquo;</li>
</ul>
<p>Small jobs can take advantage of flexible scheduling across both clusters.
Large jobs can only run on the SMP due to hardware constraints.  We were faced
with a choice for medium-sized jobs.  We could either submit them so they&rsquo;d run
on the SMP only or submit them so they&rsquo;d only run on the larger commodity
nodes.  We opted for the latter since we have more capacity on the commodity
cluster.</p>
<h2 id="next-steps">Next steps</h2>
<p>During our next maintenance window, I intend to try reworking our NUMA config
to aggregate both sockets on a node board into a single NUMA node so we&rsquo;d have
16 cores instead of 8 per NUMA node.  Assuming that works as advertised, we
should be able to submit small to medium shared memory jobs so they&rsquo;d run on
either cluster&hellip; at least until we deploy commodity nodes with more than 16
cores, which may happen as soon as this fall with the release of Ivy Bridge.</p>
<p>I&rsquo;ve had several discussions with high level folks at Adaptive about this (and
other issues related to the 4.1 series).  I&rsquo;m hopeful that I&rsquo;ve pressed the
issue strongly enough with them that they make it a priority on their
development roadmap to better address these limitations, but it&rsquo;s anyone&rsquo;s
guess when/if we&rsquo;ll see a release that provides a satisfactory resolution to my
complaints.</p>
                </section>
            </article>
            <footer id="post-meta" class="clearfix">
            
            &lt;&lt;&lt; <a class="basic-alignment left" href="https://thoughts.derekgottlieb.com/blog/2015/01/24/vps-provider-hopping/" title="Older Post: VPS provider hopping">VPS provider hopping</a>
            
            -----
            
            <a class="basic-alignment right" href="https://thoughts.derekgottlieb.com/blog/2013/04/29/btrfs-production-experiences/" title="Newer Post: Btrfs production experiences">Btrfs production experiences</a> >>>
            
            </footer>
            <footer id="post-meta" class="clearfix">
                <a href="https://twitter.com/derekgottlieb">
                        <img class="avatar" src="https://thoughts.derekgottlieb.com/images/avatar.png">
                        <div>
                            <span class="dark">D.G.</span>
                            <span>Trained in the ancient art of yak shaving</span>
                        </div>
                    </a>
                <section id="sharing">
                    <a class="twitter" href="https://twitter.com/intent/tweet?text=https%3a%2f%2fthoughts.derekgottlieb.com%2fblog%2f2013%2f05%2f09%2fnuma-and-torque-slash-moab%2f - NUMA%20and%20torque%2fMoab by @derekgottlieb"><span class="icon-twitter"> tweet</span></a>

<a class="facebook" href="#" onclick="
    window.open(
      'https://www.facebook.com/sharer/sharer.php?u='+encodeURIComponent(location.href),
      'facebook-share-dialog',
      'width=626,height=436');
    return false;"><span class="icon-facebook-rect"> Share</span>
</a>

                </section>
            </footer>

            

            <ul id="post-list" class="archive readmore">
    <h3>Read more</h3>

    
    
    
        <li>
            <a href="https://thoughts.derekgottlieb.com/blog/2021/11/24/synology-dsm7-spk-for-nomad/">Synology DSM7 SPK for Nomad<aside class="dates">Nov 24 2021</aside></a>
        </li>
    
        <li>
            <a href="https://thoughts.derekgottlieb.com/blog/2020/11/06/3d-print-extra-car-cup-holders/">3D Print: Extra Car Cup Holders<aside class="dates">Nov 6 2020</aside></a>
        </li>
    
        <li>
            <a href="https://thoughts.derekgottlieb.com/blog/2020/11/03/3d-print-usb-stylus-holder/">3D Print: USB Stylus Holder<aside class="dates">Nov 3 2020</aside></a>
        </li>
    
        <li>
            <a href="https://thoughts.derekgottlieb.com/blog/2020/11/03/3d-print-collapsible-dice-tower/">3D Print: Collapsible Dice Tower<aside class="dates">Nov 3 2020</aside></a>
        </li>
    
        <li>
            <a href="https://thoughts.derekgottlieb.com/blog/2020/11/02/3d-print-wine-bottle-bird-feeder/">3D Print: Wine Bottle Bird Feeder<aside class="dates">Nov 2 2020</aside></a>
        </li>
    
        <li>
            <a href="https://thoughts.derekgottlieb.com/blog/2020/11/02/3d-print-dragon-eggs-with-escher-lizards/">3D Print: Dragon Eggs with Escher Lizards<aside class="dates">Nov 2 2020</aside></a>
        </li>
    
        <li>
            <a href="https://thoughts.derekgottlieb.com/blog/2020/11/01/3d-print-playing-card-case/">3D Print: Playing Card Case<aside class="dates">Nov 1 2020</aside></a>
        </li>
    
        <li>
            <a href="https://thoughts.derekgottlieb.com/blog/2020/11/01/3d-print-basalt-pencil-holder/">3D Print: Basalt Pencil Holder<aside class="dates">Nov 1 2020</aside></a>
        </li>
    
        <li>
            <a href="https://thoughts.derekgottlieb.com/blog/2020/11/01/3d-print-baby-dragon-pen-holder/">3D Print: Baby Dragon Pen Holder<aside class="dates">Nov 1 2020</aside></a>
        </li>
    
        <li>
            <a href="https://thoughts.derekgottlieb.com/blog/2020/11/01/3d-print-cute-dragon/">3D Print: Cute Dragon<aside class="dates">Nov 1 2020</aside></a>
        </li>
    
</ul>

            <footer id="footer">
    
        <div id="social">

	
	
    <a class="symbol" href="https://www.github.com/derekgottlieb">
        <i class="fa fa-github"></i>
    </a>
    
    <a class="symbol" href="https://www.twitter.com/derekgottlieb">
        <i class="fa fa-twitter"></i>
    </a>
    


</div>

    
    <p class="small">
    
        © Copyright 2022 D.G.
    
    </p>
</footer>

        </section>

        <script src="//ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
<script src="https://thoughts.derekgottlieb.com/js/main.js"></script>
<script src="https://thoughts.derekgottlieb.com/js/highlight.js"></script>
<script>hljs.initHighlightingOnLoad();</script>





    </body>
</html>
